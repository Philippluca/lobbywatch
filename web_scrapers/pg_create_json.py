# -*- coding: utf-8 -*-

import csv
import json
import os
import re
import datetime
from subprocess import call
from datetime import datetime
from collections import defaultdict
from shutil import copyfile
from argparse import ArgumentParser

import literals 
import pdf_helpers 


def is_title(row):
    row = ' '.join(row)
    number = row.split('.')[0]
    return re.match('^\d+$', number)


def is_end(row):
    return "Die Liste ist ebenfalls auf Internet abrufbar" in "".join(row)


def extract_title(row):
    row = ' '.join(row).strip()
    row = row.split('.')[1]
    return row.strip()


def is_president(row):
    row = ' '.join(row).strip()
    return any(map(row.startswith, literals.president_title))


def extract_presidents(row):
    row = ' '.join(row)
    if ':' in row:
        row = row.split(':')[1]
    names = row.split(',')
    names = [name.strip() for name in names if name.strip() != '' and name.strip() != "vakant"]
    names = [name.replace("Dr.", "").replace("vakant", "").replace(";", "") for name in names]
    return names


def is_sekretariat(row):
    row = ''.join(row)
    return row.startswith('Sekretariat:') or row.startswith('Co-Sekretariat:')


def extract_sekretariat(row):
    row = ' '.join(row)
    if ':' in row and "Homepage" not in row:
        row = row.split(':')[1]
    address_rows = row.split(',')
    address_rows = [address_row.strip() for address_row in address_rows if address_row != '']
    address_rows = [address_row 
                    for index, address_row 
                    in enumerate(address_rows)
                    if not all(map(lambda row : re.sub(r'\d', '', row).strip() == "", 
                        address_rows[index:]))
                    ]
    return address_rows


# read the csv generated by tabula and parse the groups and their members
def cleanup_file(filename):
    groups = {}
    presidents = []
    sekretariat = []
    group = ''
    reading_presidents = False
    reading_sekretariat = False
    rows = csv.reader(open(filename, encoding="utf-8"))
    for row in rows:

        if is_title(row):
            if group and presidents:
                groups[group] = (presidents, sekretariat)
            presidents = []
            sekretariat = []
            group = extract_title(row)
            reading_sekretariat = False

        if is_end(row):
            if group and presidents:
                groups[group] = (presidents, sekretariat)
                break

        if is_sekretariat(row) or reading_sekretariat:
            reading_presidents = False
            reading_sekretariat = True
            sekretariat += extract_sekretariat(row)

        if is_president(row) or reading_presidents:
            presidents += (extract_presidents(row))
            reading_presidents = True
            reading_sekretariat = False


    # print counts for sanity check
    print("{} parlamentarische Gruppen\n"
          "{} total members of parlamentarische Gruppen".format(
              len(groups),
              sum(len(gruppe) for gruppe in groups.values())))

    return groups 


# write member of parliament and guests to json file
def write_to_json(groups, archive_pdf_name, filename, url, creation_date, imported_date):
    data = [{
            "name": group,
            "praesidium": members,
            "sekretariat": sekretariat
            } for group, (members, sekretariat) in groups.items()]
            
    metadata_data = {
                "metadata": {
                    "archive_pdf_name": archive_pdf_name,
                    "filename": filename,
                    "url": url,
                    "pdf_creation_date": creation_date.isoformat(' '), # , timespec is addedin Python 3.6: 'seconds'
                    "imported_date": imported_date.isoformat(' ') # , timespec is addedin Python 3.6: 'seconds'
                },
                "data": data
    }

    with open(filename, "wb") as json_file:
        contents = json.dumps(metadata_data, indent=4,
                              separators=(',', ': '),
                              ensure_ascii=False).encode("utf-8")
        json_file.write(contents)


# download a pdf containing the guest lists of members of parlament in a table
# then parse the file into json and save the json files to disk
def scrape():
    parser = ArgumentParser(description='Scarpe Parlamentarische Gruppen PDF')
    parser.add_argument("local_pdf", metavar="file", nargs='?', help="local PDF file to use", default=None)
    args = parser.parse_args()
    local_pdf = args.local_pdf

    url = "https://www.parlament.ch/centers/documents/de/parlamentarische-gruppen.pdf"
    filename = "parlamentarische-gruppen.json"

    script_path = os.path.dirname(os.path.realpath(__file__))
    try:
        import_date = datetime.now().replace(microsecond=0)
        raw_pdf_name = url.split("/")[-1]
        pdf_name = "{}-{:02d}-{:02d}-{}".format(import_date.year, import_date.month, import_date.day, raw_pdf_name)
        if local_pdf is None:
            print("\ndownloading " + url)
            pdf_helpers.get_pdf_from_admin_ch(url, pdf_name)
        else:
            print("\ncopy local PDF " + local_pdf)
            copyfile(local_pdf, pdf_name)

        print("\nextracting metadata...")
        creation_date = pdf_helpers.extract_creation_date(pdf_name)
        archive_pdf_name = "{}-{:02d}-{:02d}-{}".format(creation_date.year, creation_date.month, creation_date.day, raw_pdf_name)
        archive_filename = "{}-{:02d}-{:02d}-{}".format(creation_date.year, creation_date.month, creation_date.day, filename)
        print("\nPDF creation date: {:02d}.{:02d}.{}\n".format(creation_date.day, creation_date.month, creation_date.year))

        print("parsing PDF...")
        FNULL = open(os.devnull, 'w')
        tabula_path = script_path + "/tabula-0.9.2-jar-with-dependencies.jar"
        call(["java", "-jar", tabula_path, pdf_name, "--pages", "all", "-o", "pg_data.csv"], stderr=FNULL)

        print("cleaning up parsed data...")
        groups = cleanup_file("pg_data.csv")

        print("writing " + filename + "...")
        write_to_json(groups, archive_pdf_name, filename, url, creation_date, import_date)

        if local_pdf is None:
            print("archiving...")
            copyfile(pdf_name, script_path + "/archive/{}".format(archive_pdf_name))
            copyfile(filename, script_path + "/archive/{}".format(archive_filename))

    finally:
        print("cleaning up...")
        os.rename(pdf_name, script_path + "/backup/{}".format(pdf_name))
        backup_filename = "{}-{:02d}-{:02d}-{}".format(import_date.year, import_date.month, import_date.day, filename)
        copyfile(filename, script_path + "/backup/{}".format(backup_filename))
        os.remove("pg_data.csv")



#main method
if __name__ == "__main__":
    scrape()
